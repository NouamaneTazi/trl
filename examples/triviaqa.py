import re

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, load_tool

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, TextEnvironment


seed = 1
dataset = load_dataset("trivia_qa", "rc.wikipedia", split="train")
dataset = dataset.shuffle(seed)

dataset[0]["question"]
dataset[0]["answer"]["normalized_aliases"]


def data_generator():
    for i in range(len(dataset)):
        yield dataset[i]["question"], [item for item in dataset[i]["answer"]["normalized_aliases"]]


gen = data_generator()
gen = iter(gen)


def generate_data(n):
    tasks, answers = [], []
    for i in range(n):
        q, a = next(gen)
        tasks.append(q)
        answers.append(a)
    return tasks, answers


# g = data_generator()
# g = iter(g)
# raise
# set up models
model_id = "gpt2-medium"
model = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)
model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token


def exact_match_reward(responses, answers=None):
    """Reward if generated response contains correct answer."""
    rewards = []
    pattern = r"(?<=Result=).*(?=<submit>)"  # generated by chatGPT
    for response, answer in zip(responses, answers):
        reward = 0.0
        match_pattern = re.findall(pattern, response)
        if match_pattern:
            for a in answer:
                if match_pattern[0].lower() == a.lower():
                    reward += 1.0
        rewards.append(torch.tensor(reward))
    return rewards


# system prompt
prompt = """\
In which branch of the arts is Patricia Neary famous?

<request><Wiki>Patricia Neary<call>Patricia Neary (born October 27, 1942) is an American ballerina, choreographer and ballet director, who has been particularly active in Switzerland. She has also been a highly successful ambassador for the Balanchine Trust, bringing George Balanchine's ballets to 60 cities around the globe.<response>

Result=Ballets<submit>

Who won Super Bowl XX?

<request><Wiki>Super Bowl XX<call>Super Bowl XX was an American football game between the National Football Conference (NFC) champion Chicago Bears and the American Football Conference (AFC) champion New England Patriots to decide the National Football League (NFL) champion for the 1985 season. The Bears defeated the Patriots by the score of 46â€“10, capturing their first NFL championship (and Chicago's first overall sports victory) since 1963, three years prior to the birth of the Super Bowl. Super Bowl XX was played on January 26, 1986 at the Louisiana Superdome in New Orleans.<response>

Result=Chicago Bears<submit>

Which William wrote the novel Lord Of The Flies?

<request><Wiki>Lord Of The Flies<call>Lord of the Flies is a 1954 novel by William Golding.<response>

Result=Golding<submit>

Which William wrote the novel Lord Of The Flies?

<request><Wiki>Lord Of The Flies<call>Lord of the Flies is a 1954 novel by William Golding.<response>

Result=Golding<submit>

What was Sony's portable tape player called?

<request><Wiki>Sony's portable tape player<call>Walkman is a series of portable media players and some Xperia mobile phones manufactured by Sony. The original Walkman, released in 1979, was a portable cassette player that changed listening habits by allowing people to listen to music on the move. It was devised by Sony cofounder Masaru Ibuka, who felt Sony's existing portable player was too unwieldy and expensive. A prototype was built from a modified Sony Pressman, a compact tape recorder designed for journalists and released in 1977.<response>

Result=Walkman<submit>"""

generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "eos_token_id": -1,
    "max_new_tokens": 32,
}

# trainer
ppo_config = PPOConfig(
    batch_size=16,
    learning_rate=1.41e-5,
    mini_batch_size=16,
    log_with="wandb",
)
ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)

# text env
tool = load_tool("vwxyzjn/pyserini-wikipedia-kilt-doc")


def tool_fn(x):
    return tool(x).split("\n")[1]


text_env = TextEnvironment(
    model,
    tokenizer,
    {"Wiki": tool_fn},
    exact_match_reward,
    prompt,
    generation_kwargs=generation_kwargs,
)

# main training loop
for step in range(100):
    tasks, answers = generate_data(ppo_config.batch_size)
    queries, responses, masks, rewards, histories = text_env.run(tasks, answers=answers)
    train_stats = ppo_trainer.step(queries, responses, rewards, masks)

    response_texts = [tokenizer.decode(response) for response in responses]
    query_texts = [tokenizer.decode(query) for query in queries]
    texts = {
        "query": [qt.split("<submit>")[-1].strip() for qt in query_texts],
        "response": response_texts,
        "answer": [", ".join(item) for item in answers],
    }
    ppo_trainer.log_stats(train_stats, texts, rewards)
ppo_trainer.save_pretrained(model_id + "-calculator")
